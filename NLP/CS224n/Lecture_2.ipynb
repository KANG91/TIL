{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 2 Word Vector Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. word meaning\n",
    "1. how do we represent the meaning of a word?  \n",
    "![2-1.png](./img/2-1.png)\n",
    "- pandas와 관련있는 단어를 wordnet에서 검색하여 list한 것처럼 보임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "problems with this discrete representation\n",
    "- 단어를 표현하는 방법 - one hot encoding(zero vector에 1이 있다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![2-2.png](./img/2-2.png)\n",
    "\n",
    "- hotel과 motel을 one hot encoding하여 곱할 경우 0이 됨\n",
    "- \"They have no inherent notion of similarity\"(유사도에 대해 내재적 개념이 없다)\n",
    "- 단어와 단어간의 관계가 전혀 드러나지 않는다. 예를 들어 '강아지'와 '멍멍이'는 실제로는 유사한 의미지만, 전혀 다른 벡터로 표현이 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![2-3.png](./img/2-3.png)\n",
    "\n",
    "- 가령, banking에 대한 단어의 뜻을 알고 싶다면 banking이 포함된 모든 문장을 살펴보면 뜻을 알 수 있음.  \n",
    "- 2가지 키워드, distributional, distributed representation, 2가지가 다른 개념임을 이해해야함. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * 추가정보\n",
    "(https://dreamgonfly.github.io/machine/learning,/natural/language/processing/2017/08/16/word2vec_explained.html)\n",
    "** 단어를 벡터로 바꾸는 작업을 임베딩(Embedding)이라고 함 ** \n",
    "- distribution representation : 분산표현방식이나 임베딩(Embedding)과 유사.\n",
    "- 임베딩 방식을 사용하면, 각 카테고리를 원하는 사이즈의 실수 벡터로 표현이 가능, 나중에 모델에서 feature로써 사용함. \n",
    "- 데사스 수업시간에 왕,남자, 여자, 여왕 단어들의 벡터 관계를 +, -한 내용이 있는데 이 것은 ** 단어의 의미가 벡터에 잘 반영되어있다는 것을 뜻함 **\n",
    "- 언어의 속성은 단어 그 자체가 될 수도 있고, 품사가 될 수도 있고, 문맥의 위치가 될 수도 있는데 이러한 언어적 정보(linguistic information)을 언어의 feature representation이라고 함.\n",
    "- 언어의 속성을 표헌하는 방법으로 sparse(one hot encoding), dense representation(word2vec)이 있음.0\n",
    "- sparse representation :  \n",
    "one hot encoding과 동일, 0과 1로만 표현, 가장 단순하고 전통적으로 쓰이는 표현 방식\n",
    "- dense representation(distributed representation) :    \n",
    "각각의 속성을 독립적인 차원으로 나타내지 않고, 우리가 정한 개수의 차원으로 대상을 대응시켜 표현. ** 하나의 정보가 여러 차원에 분산되어 표현되기 때문에 distributed **  \n",
    "각각의 차원이 어떤 의미를 지니는지는 알 수 없으나, 강아지와 멍멍이가 어느정도 유사한지는 알 수 있음. -> 단어 벡터간의 의미가 드러남\n",
    "- dense representation의 장점 : \n",
    "> 1. 적은 차원으로 대상을 표현할 수 있음. sparse representation은 차원의 수가 보통 엄청나게 높아짐. 입력 데이터의 차원이 높으면 curse of dimensionality가 발생. \n",
    "> 2. 더 큰 generalizaion power를 가지고 있음. 강아지와 멍멍이 간의 유사 관계를 알기 때문에, 문장에서 강아지가 훨씬 많이 나와도 강아지에 대한 정보가 멍멍이에도 '일반화'될 수 있음. \n",
    "\n",
    "dense representation의 장점들이 잘 발현되려면? ** word embedding이 잘 학습되어야함 **  \n",
    "임베딩을 잘 학습시키기 위한 방법 중 가장 유명한 것이 ** word2vec **  \n",
    "\n",
    "word2vec의 핵심아이디어 -> ** \"you shall know a word by the company it keeps\" **\n",
    "\n",
    "## word2vec의 2가지 알고리즘\n",
    "### 1. CBOW(continuous bag of words)\n",
    "맥락(context)으로 타겟 단어(target word)를 예측하는 문제를 품.타겟 단어의 앞뒤 단어들을 주변 단어라 하며, 이 주변단어를 ** 'window' **라고 하며, 단어의 개수를 ** 'window size' ** 라고 함. CBOW는 맥락으로 단어를 예측하는 문제.  \n",
    "\n",
    "sliding window 방식\n",
    "![2-4.png](./img/2-4.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. word2vec introduction\n",
    "- word2vec 이전의 모델들에 비해 word2vec이 훨씬 간편하고 빠른 모델임을 발견\n",
    "- 2가지 알고리즘 - Skip grams, CBOW(Continuous Bag of Words)\n",
    "#### Skip grams\n",
    "![2-5](./img/2-5.png)\n",
    "- center word인 'banking'을 중심으로, 윈도우 사이즈 만큼의 옆 단어들과의 관계 고려\n",
    "- ** 한 가지 context word의 확률분포 인것을 아는 것이 중요 ** \n",
    "\n",
    "#### details of word2vec\n",
    "- 두 가지의 단어를 내적하여 소프트 맥스에 넣음\n",
    "> 두 단어의 벡터는 두 벡터의 모든 요소를 곱한 것과 마찬가지이며, 두 벡터가 유사할수록 그 값이 커짐. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- picure of model\n",
    "![2-6](./img/2-6.png)\n",
    "- 웟 한 잇코딩 형태로 벡터가 input\n",
    "- output에서 softmax 단계에서 확률을 높이 주게 만든 것과 truth에서 1이 나온 부분이 다르다면 loss가 생길 것 -> 우리가 잘못한 부분 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. research highlight\n",
    "#### 'a simple but tough-to-beat baseline for sentence embeddings'\n",
    "\"어떻게 해야 단어 벡터와 문장 벡터가 meaning을 가지게 되는가?\"\n",
    "- vector representation 을 sentence representation으로 변환하는 간단한 방법 : **BOW**\n",
    "![2-7](./img/2-7.png)\n",
    "- bow를 만드는 1단계 : 단어들의 벡터에 평균을 취하나 단어들마다 가중치를 부여\n",
    "- bow를 만드는 2단계 : compute the first principal component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. word2vec objective function gradients\n",
    "(negative log likelihood, softmax 관련 공식 미분하는 진행 이해 못함 - 정확히는 공식에서 각 알파벳이 뜻하는 바를 모르겠음)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradient descent 설명 -> loss function or objective function 에 대한 optimization 을 하기 위해 중요한 것은 'alpha'의 크기. w가 최소값이 될 수 있도록 충분한 크기여야함.\n",
    "- 데이터의 크기가 엄청 커질경우 : realistic amount of time에 해결하지 못할것. \n",
    "- Stochastic Gradient descent\n",
    "- gradient descent의 노이즈가 커질 경우, nueral net에 긍정적인 영향을끼쳐 SGD가 나은 결과를 도출할 수 있도록 도와줌."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(수업 처음에 한다고 얘기는 했으나 실제로는 안한 것 같은데...)\n",
    "\n",
    "### 5. optimization refresher\n",
    "### 6. assignment 1 notes\n",
    "### 7. usefulness of word2vec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
