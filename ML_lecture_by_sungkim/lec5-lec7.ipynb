{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lec 5-1 : logistic classification 의 가설 함수 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "분류의 종류\n",
    "- 스팸 메일\n",
    "- 페이스북 피드\n",
    "- 신용카드 사기 거래\n",
    "\n",
    "1 또는 0 으로 예측. \n",
    "\n",
    "주식 시장의 상승 하락을 분류 할 수 도 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ z = Wx + b$$\n",
    "를 만들어 g(z) 가 0에서 1사이의 결과값을 내주는 함수를 찾음  \n",
    "#### sigmoid(or logistic function)  \n",
    "$$g(x) = \\frac{1}{(1+e^{-W^TX})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "항상 0과 1사이의 값을 가질 수 있게 됨.  \n",
    "logistic 함수의 가장 중요한 부분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](img/5-1.png)\n",
    "- 함수가 linear regression일 때는 cost function 의 그래프가 2차 포물선으로 비교적 단순\n",
    "- But, logistic 함수의 경우에는 cost function 의 그래프가 오른쪽과 같이 울퉁불퉁\n",
    "- logistic cost 함수의 경우 local minimum에 걸려, global minimum을 못찾을 수 있음.\n",
    "- 따라서, hypothesis를 바꿨으므로 cost function도 바꿔야함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### new cost function for logistic\n",
    "$$  cost(W) = \\frac{1}{m}\\sum c(H(x), y)$$\n",
    "$$  c(H(x), y) = \n",
    "\\begin{cases}\n",
    "-log(H(x)), &\\mbox{ : y = 1} \\\\\n",
    "-log(1 - H(x)), &\\mbox{ : y = 0}\n",
    "\\end{cases}$$\n",
    "\n",
    "c함수는 y 가 1, 0일 경우로 2가지로 나눠서 정의.  \n",
    "exp로 구부러진 함수를 log함수로 잡는 것이 기본적인 아이디어\n",
    "\n",
    "$$C(H(x), y) = ylog(H(x)) - (1-y)log(1-H(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimize cost - Gradient descent algorithm\n",
    "가중치에서 alpha를 곱해준 cost(W)의 미분값을 빼줌  \n",
    "(텐서플로우의 경우 gradientdescentoptimizer 를 사용하면 자동 해결)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lec 6-1 : Softmax regression 기본 개념 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logistic regression\n",
    "- 기존의 linear regression 의 형태는 binary classification 이 안됨  \n",
    "- 0과 1로 결과값이 나올 수 있는 함수를 고안 -> sigmoid or logistic 을 제안\n",
    "- 그림으로 표현할 경우 아래와 같음\n",
    "![image.png](img/5-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](img/5-3.png)\n",
    "- 학점 A,B,C를 주는 경우\n",
    "- A 이거나 아니거나 , B이거나 아니거나, C이거나 아니거나 3개의 각각 다른 선을 그어 구분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](img/5-4.png)\n",
    "- 위와 같이 각각 3개의 공식을 만들어 3개의 선을 그을 수 있으나 공식 및 코드가 복잡해짐\n",
    "- 하나의 matrix 형태로 만들어서 곱을 할 경우, 위와 같이 3 by 1 형태의 vector형태로 결과값이 나옴\n",
    "- 최종 벡터 값에서 A, B, C 의 값 중 하나를 골라 주는 것이 softmax 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### softmax function\n",
    "![image.png](img/5-5.png)\n",
    "- 최종 벡터값(가장 좌측)에서 시그모이드 함수를 적용 - 이를 다시 one-hot-encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cost function\n",
    "![image.png](img/5-6.png)\n",
    "- cross entropy\n",
    "- L이 실제 정답, S(Y)는 y hat, 두 개사이의 차이가 얼마나 되는지 크로스 엔트로피 함수를 통해 구하게 됨. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lab 06-1 : TensorFlow로 softmax classification 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_data = [[1, 2, 1, 1], [2,1, 3, 2], [3,1,3,4], [4,1,5,5], [1,7,5,5,],[1,2,5,6], [1,6,6,6], [1,7,7,7]]\n",
    "y_data = [[0,0,1], [0,0,1], [0,0,1], [0,1,0], [0,1,0], [0,1,0], [1,0,0], [1,0,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X  =tf.placeholder(\"float\", [None, 4])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "nb_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_normal([4, nb_classes]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name = 'bias')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis = 1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess :\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(3001) :\n",
    "        sess.run(optimizer, feed_dict = {X: x_data, Y:y_data})\n",
    "        if step % 200 == 0 :\n",
    "            print(step, sess.run(cost, feed_dict = {X: x_data, Y: y_data}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = sess.run(hypothesis, feed_dict = {X : [[1, 11, 7, 9]]})\n",
    "print(a, sess.run(tf.arg_max(a, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lab 06 -2 : TensorFlow Fancy Softmax classification 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#logistic classifier 통과 후\n",
    "logits = tf.matmul(X, W) + b\n",
    "#softmax function 을 통해 one-hot-encoding \n",
    "hypothesis = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost_i = tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = Y_one_hot)\n",
    "cost = tf.reduce_mean(cost_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = tf.placeholder(tf.int32, [None, 1]) # 0 ~ 6, shape = (? , 1)\n",
    "Y_one_hot = tf.one_hot(Y, nb_classes) # one hot shape = (?, 1, 7)\n",
    "#위의  one_hot 만 했을 경우, 창원의 수가 추가 되어 (?, 1, 7)의 형태가 나옴\n",
    "#우리가 원하는 형태가 아니므로, tf.reshape로 다시 one_hot 형태를 (?, 7)로 변겅\n",
    "Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes]) #shape = (?, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lec 07 - 1 : 학습 rate, overfitting, regularization\n",
    "\n",
    "- 경사를 따라 내려가는 step 클 경우 - ** overshooting **\n",
    "- 경사를 따라 내려가는 step이 작은 경우 - **small learning rate **\n",
    "> global minimum에 도달하지 못하고 local minimum에서 멈춰버림\n",
    "\n",
    "learning rate를 정하는 것에 답은 없음. 데이터에 따라 다름  \n",
    "시작은 0.01로 하는 것 권장\n",
    "\n",
    "데이터를 전처리해야 하는 경우 \n",
    "![image.png](img/7-1.png)\n",
    "- 데이터값의 큰 차이가 있어, 등고선이 원이 아닌 타원의 형태를 띄게 됨\n",
    "- learning rate를 하던 도중, 바깥으로 틩겨져 나가 버릴 수 있음.\n",
    "- learning rate에 문제가 없는 것 같으나, cost가 발산하는 것 같다면 데이터 중 값이 크게 차이가 나는 것이 있는지 확인할 것!\n",
    "\n",
    "#### standardization\n",
    "- 값에서 평균을 빼고, 표준편차로 나눠준 값\n",
    "\n",
    "#### overfitting\n",
    "- training data에 너무 잘 맞는 형태로 모델이 만들어지는 것. -> 실제 상황에서 잘 안 맞을수도\n",
    "\n",
    "over fitting 을 줄이기 위해선?\n",
    "- more training data!\n",
    "- reduce number of features\n",
    "- regularization\n",
    "\n",
    "regularization이란?  \n",
    "- Let's not have too big numbers in the weight\n",
    "- lidge, lasso, elastic 등 배운 내용\n",
    "- tensorflow code 로는 ** l2reg = 0.001 * tf.reduce_sum(tf.square(W)) **\n",
    "\n",
    "### lec 07 - 2 : Training/Testing 데이타 셋\n",
    "\n",
    "- 전체 데이터 셋을 train/test로 나누는 부분에 대한 설명\n",
    "- 이론과 실제 구현까지 가능하므로 패스\n",
    "\n",
    "#### online learinng\n",
    "- train set 이 100만개가 있을 때, 10만개씩 잘라서 학습을 시키는 것. \n",
    "- 첫 번째 데이터 셋의 학습 결과가 모델에 남아있어야 함. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
