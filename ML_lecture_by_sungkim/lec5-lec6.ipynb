{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lec 5-1 : logistic classification 의 가설 함수 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "분류의 종류\n",
    "- 스팸 메일\n",
    "- 페이스북 피드\n",
    "- 신용카드 사기 거래\n",
    "\n",
    "1 또는 0 으로 예측. \n",
    "\n",
    "주식 시장의 상승 하락을 분류 할 수 도 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ z = Wx + b$$\n",
    "를 만들어 g(z) 가 0에서 1사이의 결과값을 내주는 함수를 찾음  \n",
    "#### sigmoid(or logistic function)  \n",
    "$$g(x) = \\frac{1}{(1+e^{-W^TX})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "항상 0과 1사이의 값을 가질 수 있게 됨.  \n",
    "logistic 함수의 가장 중요한 부분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](img/5-1.png)\n",
    "- 함수가 linear regression일 때는 cost function 의 그래프가 2차 포물선으로 비교적 단순\n",
    "- But, logistic 함수의 경우에는 cost function 의 그래프가 오른쪽과 같이 울퉁불퉁\n",
    "- logistic cost 함수의 경우 local minimum에 걸려, global minimum을 못찾을 수 있음.\n",
    "- 따라서, hypothesis를 바꿨으므로 cost function도 바꿔야함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### new cost function for logistic\n",
    "$$  cost(W) = \\frac{1}{m}\\sum c(H(x), y)$$\n",
    "$$  c(H(x), y) = \n",
    "\\begin{cases}\n",
    "-log(H(x)), &\\mbox{ : y = 1} \\\\\n",
    "-log(1 - H(x)), &\\mbox{ : y = 0}\n",
    "\\end{cases}$$\n",
    "\n",
    "c함수는 y 가 1, 0일 경우로 2가지로 나눠서 정의.  \n",
    "exp로 구부러진 함수를 log함수로 잡는 것이 기본적인 아이디어\n",
    "\n",
    "$$C(H(x), y) = ylog(H(x)) - (1-y)log(1-H(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimize cost - Gradient descent algorithm\n",
    "가중치에서 alpha를 곱해준 cost(W)의 미분값을 빼줌  \n",
    "(텐서플로우의 경우 gradientdescentoptimizer 를 사용하면 자동 해결)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lec 6-1 : Softmax regression 기본 개념 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logistic regression\n",
    "- 기존의 linear regression 의 형태는 binary classification 이 안됨  \n",
    "- 0과 1로 결과값이 나올 수 있는 함수를 고안 -> sigmoid or logistic 을 제안\n",
    "- 그림으로 표현할 경우 아래와 같음\n",
    "![image.png](img/5-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](img/5-3.png)\n",
    "- 학점 A,B,C를 주는 경우\n",
    "- A 이거나 아니거나 , B이거나 아니거나, C이거나 아니거나 3개의 각각 다른 선을 그어 구분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](img/5-4.png)\n",
    "- 위와 같이 각각 3개의 공식을 만들어 3개의 선을 그을 수 있으나 공식 및 코드가 복잡해짐\n",
    "- 하나의 matrix 형태로 만들어서 곱을 할 경우, 위와 같이 3 by 1 형태의 vector형태로 결과값이 나옴\n",
    "- 최종 벡터 값에서 A, B, C 의 값 중 하나를 골라 주는 것이 softmax 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### softmax function\n",
    "![image.png](img/5-5.png)\n",
    "- 최종 벡터값(가장 좌측)에서 시그모이드 함수를 적용 - 이를 다시 one-hot-encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cost function\n",
    "![image.png](img/5-6.png)\n",
    "- cross entropy\n",
    "- L이 실제 정답, S(Y)는 y hat, 두 개사이의 차이가 얼마나 되는지 크로스 엔트로피 함수를 통해 구하게 됨. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lab 06-1 : TensorFlow로 softmax classification 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_data = [[1, 2, 1, 1], [2,1, 3, 2], [3,1,3,4], [4,1,5,5], [1,7,5,5,],[1,2,5,6], [1,6,6,6], [1,7,7,7]]\n",
    "y_data = [[0,0,1], [0,0,1], [0,0,1], [0,1,0], [0,1,0], [0,1,0], [1,0,0], [1,0,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X  =tf.placeholder(\"float\", [None, 4])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "nb_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_normal([4, nb_classes]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name = 'bias')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis = 1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess :\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(3001) :\n",
    "        sess.run(optimizer, feed_dict = {X: x_data, Y:y_data})\n",
    "        if step % 200 == 0 :\n",
    "            print(step, sess.run(cost, feed_dict = {X: x_data, Y: y_data}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sess.run(hypothesis, feed_dict = {X : [[1, 11, 7, 9]]})\n",
    "print(a, sess.run(tf.arg_max(a, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lab 06 -2 : TensorFlow Fancy Softmax classification 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic classifier 통과 후\n",
    "logits = tf.matmul(X, W) + b\n",
    "#softmax function 을 통해 one-hot-encoding \n",
    "hypothesis = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_i = tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = Y_one_hot)\n",
    "cost = tf.reduce_mean(cost_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = tf.placeholder(tf.int32, [None, 1]) # 0 ~ 6, shape = (? , 1)\n",
    "Y_one_hot = tf.one_hot(Y, nb_classes) # one hot shape = (?, 1, 7)\n",
    "#위의  one_hot 만 했을 경우, 창원의 수가 추가 되어 (?, 1, 7)의 형태가 나옴\n",
    "#우리가 원하는 형태가 아니므로, tf.reshape로 다시 one_hot 형태를 (?, 7)로 변겅\n",
    "Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes]) #shape = (?, 7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
